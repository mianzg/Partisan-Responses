{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Partisan-Responses-master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_full_speech(filepath):\n",
    "    \"\"\"\n",
    "    Load into a dataframe the data from a text file in hein-bound\n",
    "    :param filepath: filepath\n",
    "    :return: speeches dataframe\n",
    "    \"\"\"\n",
    "    with open(filepath, errors=\"ignore\") as f:\n",
    "        speech = f.readlines()\n",
    "        speech = [s.strip() for s in speech]\n",
    "        speech = [[s[:s.find('|')], s[s.find('|') + 1:]] for s in speech]\n",
    "        speech_df = pd.DataFrame(speech[1:], columns=speech[0])\n",
    "    return speech_df\n",
    "\n",
    "def read_speakermap(filepath):\n",
    "    \"\"\"\n",
    "    Load into a dataframe the speaker map from a text file in hein-bound\n",
    "    :param filepath: filepath\n",
    "    :return: speakermap dataframe\n",
    "    \"\"\"\n",
    "    with open(filepath, errors=\"ignore\") as f:\n",
    "        speakermap_df = pd.read_table(f, delimiter = \"|\")\n",
    "    return speakermap_df\n",
    "\n",
    "def merge_speech_speaker(speech_df, speaker_df):\n",
    "    \"\"\"\n",
    "    Merge a dataframe containing speeches with one containing the speakermap\n",
    "    :param speech_df: speeches dataframe\n",
    "    :param speaker_df: speakermap dataframe\n",
    "    :return: merged dataframe\n",
    "    \"\"\"\n",
    "    speech_df = speech_df.astype({\"speech_id\": type(speaker_df.loc[:,'speech_id'][0])})\n",
    "    return speaker_df.merge(speech_df, on=\"speech_id\", how=\"left\")\n",
    "\n",
    "def get_speeches_filename(idx):\n",
    "    \"\"\"\n",
    "    Returns the name of the speeches file given an index\n",
    "    :param idx: index\n",
    "    :return: filename\n",
    "    \"\"\"\n",
    "    return \"speeches_{}.txt\".format(idx)\n",
    "\n",
    "def get_speakermap_filename(idx):\n",
    "    \"\"\"\n",
    "    Returns the name of the speakermap file given an index\n",
    "    :param idx: index\n",
    "    :return: filename\n",
    "    \"\"\"\n",
    "    return \"{}_SpeakerMap.txt\".format(idx)\n",
    "def change_comma(speech):\n",
    "    \"\"\"\n",
    "    Fixes comma issues due to OCR errors\n",
    "    :param speech: text of the speech\n",
    "    :return: corrected text\n",
    "    \"\"\"\n",
    "    return re.sub(\"\\.(?=\\s[a-z0-9]|\\sI[\\W\\s])\", \",\", speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare topic phrase and stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"../hein-bound/topic_phrases.txt\"\n",
    "topic_phrases = pd.read_table(DATAPATH, sep = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7555\n",
      "     topic           phrase\n",
      "0  alcohol     abus alcohol\n",
      "1  alcohol     alcohol abus\n",
      "2  alcohol  alcohol beverag\n",
      "3  alcohol  alcohol content\n",
      "4  alcohol     alcohol drug\n"
     ]
    }
   ],
   "source": [
    "print(len(topic_phrases))\n",
    "print(topic_phrases.head())\n",
    "topic_words = set(topic_phrases['phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get questions and answers from all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 141407/141407 [14:41<00:00, 160.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished file 106. 58394 pairs found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 116415/116415 [11:44<00:00, 165.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished file 107. 47803 pairs found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 125664/125664 [12:26<00:00, 168.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished file 108. 52659 pairs found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 119506/119506 [12:10<00:00, 163.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished file 109. 52532 pairs found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 133182/133182 [13:47<00:00, 160.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished file 110. 58011 pairs found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 112550/112550 [11:01<00:00, 170.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished file 111. 52966 pairs found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'01:16:03'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "questions = []\n",
    "parties = []\n",
    "for i in range(106, 112):  #final range?\n",
    "    edition = str(i).zfill(3)\n",
    "    no = 0\n",
    "\n",
    "    # read speeches \n",
    "    speech_df = read_full_speech(get_speeches_filename(edition))\n",
    "    speaker_df = read_speakermap(get_speakermap_filename(edition))\n",
    "    final_df = merge_speech_speaker(speech_df, speaker_df)\n",
    "\n",
    "    for j in tqdm(range(len(final_df))):\n",
    "        # change comma in speech (OCR error) \n",
    "        question = change_comma(str(final_df.iloc[j].speech))\n",
    "        # tokenize question speech\n",
    "        quest_sents = nltk.sent_tokenize(question)\n",
    "        # select question sentences from speech\n",
    "        party=final_df.iloc[j].party\n",
    "        for q in quest_sents:\n",
    "            words = word_tokenize(q)\n",
    "            words = [ps.stem(w.lower()) for w in words]\n",
    "            stemmed_q = ' '.join(words)\n",
    "            if any(phrase in stemmed_q for phrase in topic_words):\n",
    "                questions.append(question)\n",
    "                parties.append(party)\n",
    "                no += 1\n",
    "                break          \n",
    "    print(\"finished file {}. {} pairs found\".format(edition, no))\n",
    "df = pd.DataFrame(list(zip(questions,parties)), columns =['Speech', 'Party'])\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"all_speeches.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Mr. President, as all Members are aware, I hav...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Mr. President, I send five Senate resolutions ...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mr. President, during the impeachment trial of...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Mr. President, I appreciate the statement of t...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Mr. President, I ask unanimous consent that ac...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Mr. President, for those of us granted the rar...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Mr. President, I want to welcome all the Senat...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Mr. President, I wanted to take just a moment ...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>President Clinton has proposed at a meeting I ...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Mr. President, it is a great honor and privile...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Mr. President, although the late Honorable Har...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Mr. President, today I want to call attention ...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Mr. President, I rise to pay homage to Sergean...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Mr. President, since 1983, the United States C...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Mr. President, I rise today to pay tribute to ...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Questions Party\n",
       "0   Mr. President, as all Members are aware, I hav...     R\n",
       "1   Mr. President, I send five Senate resolutions ...     R\n",
       "2   Mr. President, during the impeachment trial of...     R\n",
       "3   Mr. President, I appreciate the statement of t...     D\n",
       "4   Mr. President, I ask unanimous consent that ac...     R\n",
       "5   Mr. President, for those of us granted the rar...     D\n",
       "6   Mr. President, I want to welcome all the Senat...     R\n",
       "7   Mr. President, I wanted to take just a moment ...     D\n",
       "8   President Clinton has proposed at a meeting I ...     D\n",
       "9   Mr. President, it is a great honor and privile...     R\n",
       "10  Mr. President, although the late Honorable Har...     R\n",
       "11  Mr. President, today I want to call attention ...     D\n",
       "12  Mr. President, I rise to pay homage to Sergean...     R\n",
       "13  Mr. President, since 1983, the United States C...     D\n",
       "14  Mr. President, I rise today to pay tribute to ...     R"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"all_speeches.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sentences = []\n",
    "for i in range(len(df)):\n",
    "    answer = df.iloc[i].Questions\n",
    "    sents = nltk.sent_tokenize(answer)\n",
    "    no_sentences.append(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11050.,  9800.,  8985.,  9090.,  9570., 10249., 10791., 11160.,\n",
       "        11280., 10715., 10292.,  9726.,  9509.,  8752.,  8327.,  7891.,\n",
       "         7300.,  6904.,  6632.,  6228.,  5970.,  5503.,  5176.,  4972.,\n",
       "         4739.,  4486.,  4235.,  4114.,  3784.,  3680.,  3425.,  3318.,\n",
       "         3100.,  2988.,  2814.,  2669.,  2439.,  2452.,  2195.,  2054.,\n",
       "         2049.,  1944.,  1864.,  1691.,  1596.,  1570.,  1503.,  1414.,\n",
       "         1283.,     0.,  1276.,  1211.,  1087.,  1069.,  1004.,  1031.,\n",
       "          859.,   871.,   895.,   794.,   831.,   782.,   716.,   664.,\n",
       "          727.,   675.,   660.,   586.,   542.,   599.,   578.,   475.,\n",
       "          543.,   448.,   441.,   462.,   458.,   402.,   402.,   397.,\n",
       "          373.,   368.,   346.,   363.,   333.,   324.,   327.,   312.,\n",
       "          296.,   278.,   266.,   295.,   284.,   251.,   218.,   252.,\n",
       "          250.,   235.,   213.,   234.]),\n",
       " array([  2.  ,   2.98,   3.96,   4.94,   5.92,   6.9 ,   7.88,   8.86,\n",
       "          9.84,  10.82,  11.8 ,  12.78,  13.76,  14.74,  15.72,  16.7 ,\n",
       "         17.68,  18.66,  19.64,  20.62,  21.6 ,  22.58,  23.56,  24.54,\n",
       "         25.52,  26.5 ,  27.48,  28.46,  29.44,  30.42,  31.4 ,  32.38,\n",
       "         33.36,  34.34,  35.32,  36.3 ,  37.28,  38.26,  39.24,  40.22,\n",
       "         41.2 ,  42.18,  43.16,  44.14,  45.12,  46.1 ,  47.08,  48.06,\n",
       "         49.04,  50.02,  51.  ,  51.98,  52.96,  53.94,  54.92,  55.9 ,\n",
       "         56.88,  57.86,  58.84,  59.82,  60.8 ,  61.78,  62.76,  63.74,\n",
       "         64.72,  65.7 ,  66.68,  67.66,  68.64,  69.62,  70.6 ,  71.58,\n",
       "         72.56,  73.54,  74.52,  75.5 ,  76.48,  77.46,  78.44,  79.42,\n",
       "         80.4 ,  81.38,  82.36,  83.34,  84.32,  85.3 ,  86.28,  87.26,\n",
       "         88.24,  89.22,  90.2 ,  91.18,  92.16,  93.14,  94.12,  95.1 ,\n",
       "         96.08,  97.06,  98.04,  99.02, 100.  ]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(no_sentences, bins=100, range=(2, 100), log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Removal\n",
    "In this section, we continue preprocess speeches with the aim of removing administrative phrases and sentencese, e.g. \"Mr. Speker\", \"I reserve the balance of my time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_wordy(s, wordy_list):\n",
    "    for i in wordy_list:\n",
    "        s=s.replace(i, \"\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove short sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_short_sent = df['Speech'].apply(lambda s: s.split(\",\")).explode().value_counts()\n",
    "repeat_short_sent = repeat_short_sent[repeat_short_sent>=10]\n",
    "repeat_short_sent_df = pd.DataFrame(data = {\"sent\":repeat_short_sent.index, \"counts\":repeat_short_sent})\n",
    "repeat_short_sent_df = repeat_short_sent_df[repeat_short_sent_df['sent'].apply(lambda x: len(x.split())>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = np.quantile(repeat_short_sent_df['counts'], 0.99)\n",
    "to_remove = repeat_short_sent_df['sent'][repeat_short_sent_df['counts']>=thres].tolist()\n",
    "to_remove = [i+\",\" for i in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speech']  = df['Speech'].apply(lambda s: remove_wordy(s,to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further remove long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_long_sent = df['Speech'].apply(lambda s: [\" \".join(i.split()) for i in s.split(\".\") if len(i.split())>1]).explode().value_counts()\n",
    "repeat_long_sent = repeat_long_sent[repeat_long_sent>=10]\n",
    "thres = np.quantile(repeat_long_sent, 0.99)\n",
    "to_remove_long = repeat_long_sent[repeat_long_sent>=thres].index.tolist()\n",
    "to_remove_long = [i+\".\" for i in to_remove_long]\n",
    "to_remove_long[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speech']  = df['Speech'].apply(lambda s: remove_wordy(s,to_remove_long))\n",
    "df['Speech'] = df['Speech'].apply(lambda s: \" \".join(s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 1-sent or 2-sent Speeches\n",
    "speechlen = df[\"Speech\"].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "final_df = df[speechlen > 2]\n",
    "pickle.dump(final_df, \"all_speech_sentence_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result in 298792 speeches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
