{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GraphWriter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUENBdua6BzddLc+SwHHh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rostro36/Partisan-Responses/blob/master/06_GraphWriter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_lpeRFa34jd",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_JcyaEJb7BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Partisan-Responses-master')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dFdfU1a38rz",
        "colab_type": "text"
      },
      "source": [
        "Install all requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48m_6NnTcA_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install torchtext -U\n",
        "!pip install spacy==2.1.0\n",
        "!pip install neuralcoref allennlp hnswlib allennlp-models\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqzaqs5Q68sV",
        "colab_type": "text"
      },
      "source": [
        "Import everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ygv1emO4Ki0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import neuralcoref\n",
        "import spacy\n",
        "import re\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import gc\n",
        "from Answer import Answer\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hle7707K7JBV",
        "colab_type": "text"
      },
      "source": [
        "Setup everything for coref"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgAs3DSn7GKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "identifier='coref'\n",
        "last_check=0\n",
        "step_size=1000\n",
        "\n",
        "def load_pickles(identifier, checkpoint):\n",
        "  if os.path.exists(\"df\"+identifier+str(checkpoint)+\".pickle\"):\n",
        "    df = pd.read_pickle(\"df\"+identifier+str(checkpoint)+\".pickle\")\n",
        "  return df\n",
        "\n",
        "def dump_pickles(identifier, checkpoint, df):\n",
        "  df.to_pickle(\"df\"+identifier+str(checkpoint)+\".pickle\")\n",
        "  gc.collect()\n",
        "  return \n",
        "\n",
        "def make_corefs(content):\n",
        "    content=str(re.sub(\"\\.(?=\\s[a-z0-9]|\\sI[\\W\\s])\", \",\", content))\n",
        "    doc=nlp(content)\n",
        "    return doc._.coref_resolved\n",
        "\n",
        "df = pd.read_pickle(\"final_data.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdpMgaKm7RwM",
        "colab_type": "text"
      },
      "source": [
        "Do coref"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKZijOCs7gQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(last_check+1,int(df.shape[0]/step_size)):\n",
        "  print(i)\n",
        "  for j in range(step_size):\n",
        "    df.iloc[step_size*(i-1)+j]['answer']=make_corefs(df.iloc[step_size*(i-1)+j]['answer'])\n",
        "  dump_pickles(identifier,i,df)\n",
        "print('done with ordered')\n",
        "for j in range(df.shape[0]-int(df.shape[0]/step_size)*step_size):\n",
        "  df.iloc[step_size*int(df.shape[0]/step_size)+j]['answer']=make_corefs(df.iloc[step_size*int(df.shape[0]/step_size)+j]['answer'])\n",
        "df.to_pickle(identifier+\".pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcG4Kx3i89Ar",
        "colab_type": "text"
      },
      "source": [
        "Setup for parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NOPFTwb7vyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "identifier=identifier\n",
        "last_check=1\n",
        "step_size=100\n",
        "#df is still from the cell above above\n",
        "\n",
        "def parse_entry(entry,verb_dict,verb_list):\n",
        "    result=dict()\n",
        "    result['question']=' '.join([token.text for token in utils.sp(entry['question'])])\n",
        "    phrase_corpus, triplet_id, parsed_text,parsed=Answer(entry['answer']).create_training(verb_dict,verb_list)\n",
        "    result['corpus']=' ; '.join(phrase_corpus)\n",
        "    result['tags']=' '.join(['<phrase>']*len(phrase_corpus))\n",
        "    result['triplet_id']=' ; '.join([re.sub('\\,','',str(x))[1:-1] for x in triplet_id])\n",
        "    result['parsed_text']=parsed_text\n",
        "    result['parsed']=' '.join([str(x) for x in parsed])\n",
        "    return result\n",
        "\n",
        "def load_pickles(identifier, checkpoint):\n",
        "  if os.path.exists(\"verb_dict\"+identifier+str(checkpoint)+\".pickle\"):\n",
        "    with open('verb_dict'+identifier+str(checkpoint)+'.pickle', 'rb') as handle:\n",
        "      verb_dict = pickle.load(handle)\n",
        "    with open('verb_list'+identifier+str(checkpoint)+'.pickle', 'rb') as handle:\n",
        "      verb_list = pickle.load(handle)\n",
        "    with open('result'+identifier+str(checkpoint)+'.pickle', 'rb') as handle:\n",
        "      result = pickle.load(handle)\n",
        "  else:\n",
        "    print('File does not exist yet.')\n",
        "    verb_dict=dict()\n",
        "    verb_list=[]\n",
        "    result=[]\n",
        "  return verb_dict, verb_list, result\n",
        "\n",
        "def dump_pickles(identifier, checkpoint, verb_dict, verb_list, result):\n",
        "  with open(\"verb_dict\"+identifier+str(checkpoint)+\".pickle\", 'wb') as handle:\n",
        "      pickle.dump(verb_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  with open(\"verb_list\"+identifier+str(checkpoint)+\".pickle\", 'wb') as handle:\n",
        "      pickle.dump(verb_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  with open(\"result\"+identifier+str(checkpoint)+\".pickle\", 'wb') as handle:\n",
        "      pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  return \n",
        "\n",
        "verb_dict, verb_list, result=load_pickles(identifier, last_check)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFSGYiCU9gat",
        "colab_type": "text"
      },
      "source": [
        "Process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0om52TA9et4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(last_check+1,int(df.shape[0]/step_size)):\n",
        "  print(i)\n",
        "  df[step_size*(i-1):step_size*i].apply(lambda x: result.append(parse_entry(x,verb_dict,verb_list)), axis=1)\n",
        "  dump_pickles(identifier,i,verb_dict,verb_list,result)\n",
        "print('done with ordered')\n",
        "df[step_size*i:].apply(lambda x: result.append(parse_entry(x,verb_dict,verb_list)), axis=1)\n",
        "dump_pickles(identifier,1+df.shape[0]/step_size,verb_dict,verb_list,result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxmENzAm8Z5a",
        "colab_type": "text"
      },
      "source": [
        "Post-processing of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN-zxpPs8ZKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame(result) \n",
        "df=df.sample(frac=1, random_state=36)\n",
        "df=df.dropna(axis=0, how='any')\n",
        "df=df[df.iloc[:,4].apply(lambda x: len(x)<1000)]\n",
        "df=df[df.iloc[:,1].apply(lambda x: len(x)<1000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1wsMNkf8PHk",
        "colab_type": "text"
      },
      "source": [
        "Write to variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKhXCZ6r8OKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "identifier='../GraphWriter-master/data/preprocessed'\n",
        "m=df.iloc[:50]\n",
        "m.to_csv(identifier+'.control.tsv', sep='\\t', index=False, header=False)\n",
        "m=df.iloc[50:1050]\n",
        "m.to_csv(identifier+'.val.tsv', sep='\\t', index=False, header=False)\n",
        "m=df.iloc[1050:2050]\n",
        "m.to_csv(identifier+'.test.tsv', sep='\\t', index=False, header=False)\n",
        "m=df.iloc[2050:]\n",
        "m.to_csv(identifier+'.train.tsv', sep='\\t', index=False, header=False)\n",
        "\n",
        "with open(identifier+'.vocab', 'w') as filehandle:\n",
        "    filehandle.writelines(\"%s\\n\" % verb.upper() for verb in verb_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hI-lDTb-thC",
        "colab_type": "text"
      },
      "source": [
        "Train with GraphWriter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJEQ67DecD4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ../GraphWriter-master/train.py -save ../GraphWriter-master/partisanResponses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zC3KRld2_YH",
        "colab_type": "text"
      },
      "source": [
        "Use file with lowest vloss in the folder \"partisanResponses\" of GraphWriter-master to generate responses on test data.\n",
        "\n",
        "\n",
        "E.g. in the next cell \"9.vloss-3.980539.lr-0.1\" was used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek1BF81C7h1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ../GraphWriter-master/generator.py -save=../GraphWriter-master/partisanResponses/9.vloss-3.980539.lr-0.1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}